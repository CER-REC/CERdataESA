{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "All paths are accessible.\n"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from sqlalchemy import text, create_engine\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pdf_files_folder = Path(\"//luxor/data/branch/Environmental Baseline Data\\Version 4 - Final/PDF\")\n",
    "csv_tables_folder = Path(\"//luxor/data/branch/Environmental Baseline Data\\Version 4 - Final/all_csvs\")\n",
    "\n",
    "if not pdf_files_folder.exists():\n",
    "    print(pdf_files_folder, \"does not exist!\")\n",
    "elif not csv_tables_folder.exists():\n",
    "    print(csv_tables_folder, \"does not exist!\")\n",
    "else:\n",
    "    print(\"All paths are accessible.\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1200)\n",
    "\n",
    "load_dotenv(override=True)\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "database = os.getenv(\"DB_DATABASE\")\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASS\")\n",
    "engine_string = f\"mysql+mysqldb://{user}:{password}@{host}/{database}?charset=utf8\"\n",
    "engine = create_engine(engine_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True c:\\Users\\T1Ivan\\Desktop\\GitHub\\CERdataESA\\Codes\\DB_setup_and_CSVs_extraction\n"
    }
   ],
   "source": [
    "# index2 = Path(\"../Input Files/Index 2 - PDFs for Major Projects with ESAs.xlsx\")\n",
    "index2 = Path().parent.parent.parent.absolute()\n",
    "print(index2.exists(), index2)\n",
    "# df = pd.read_excel(index2)\n",
    "# print(df.head())\n",
    "# df = df[[\"DataID\", \"application_name\", \"Application title short\", \"short_name\", \"Commodity\"]]\n",
    "# df = df.rename(\n",
    "#     columns={\"DataID\": \"pdfId\", \"Application title short\": \"application_title_short\", \"Commodity\": \"commodity\"})\n",
    "\n",
    "# with engine.connect() as conn:\n",
    "#     for row in df.itertuples():\n",
    "#         # print()\n",
    "#         # print(row)\n",
    "#         # print(row[\"application_name\"])\n",
    "#         stmt = text(\n",
    "#             \"UPDATE esa.pdfs SET application_name = :application_name, \" +\n",
    "#             \"application_title_short = :application_title_short, short_name = :short_name, commodity = :commodity \" +\n",
    "#             \"WHERE pdfId = :pdfId;\")\n",
    "#         params = {\"application_name\": row.application_name, \"application_title_short\": row.application_title_short,\n",
    "#                   \"short_name\": row.short_name, \"commodity\": row.commodity, \"pdfId\": row.pdfId}\n",
    "#         result = conn.execute(stmt, params)\n",
    "#         if result.rowcount != 1:\n",
    "#             print(f\"{row.pdfId}: ERROR! Updated {result.rowcount} rows!\")\n",
    "# print(\"All done\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL! DELETES ALL CSV files and CSV DB entries, and resets PDFs (csvsExtracted = NULL)!\n",
    "# with engine.connect() as conn:\n",
    "#     result = conn.execute(\"DELETE FROM esa.csvs;\")\n",
    "#     print(f\"Deleted {result.rowcount} csvs from DB\")\n",
    "#     result = conn.execute(\"UPDATE esa.pdfs SET csvsExtracted = NULL WHERE csvsExtracted IS NOT NULL;\")\n",
    "#     print(f\"Reset {result.rowcount} PDFs from DB (csvsExtracted = NULL)\")\n",
    "# csvs = list(csv_tables_folder.glob(\"*.csv\"))\n",
    "# for f in csvs:\n",
    "#     f.unlink()\n",
    "# print(f\"Deleted {len(csvs)} CSV files\")\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_args_for_csv_extraction():\n",
    "    statement = text(\"SELECT * FROM esa.pdfs WHERE csvsExtracted IS NULL ORDER BY totalPages DESC;\")\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(statement, conn)\n",
    "    pdfs = df.to_dict(\"records\")\n",
    "\n",
    "    files = []\n",
    "    for pdf in pdfs:\n",
    "        files.append(\n",
    "            (pdf[\"pdfId\"],\n",
    "            int(pdf[\"totalPages\"]),\n",
    "            engine_string,\n",
    "            str(pdf_files_folder),\n",
    "            str(csv_tables_folder)))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting CSV from PDFs\n",
    "from external import extract_csv\n",
    "\n",
    "log_file = \"log.txt\"\n",
    "with Path(log_file).open(\"w\") as f:\n",
    "    pass # Clearing the log file\n",
    "def log_it(s):\n",
    "    with Path(log_file).open(\"a\", encoding=\"utf-8-sig\") as f:\n",
    "        f.write(s)\n",
    "    print(s)\n",
    "\n",
    "start_time = time.time()\n",
    "files = create_args_for_csv_extraction()[:]\n",
    "time_stamp = time.strftime(\"%H:%M:%S %Y-%m-%d\")\n",
    "log_it(f\"Items to process: {len(files)} at {time_stamp}\\n\")\n",
    "\n",
    "with Pool() as pool:\n",
    "    results = pool.map(extract_csv, files, chunksize=1)\n",
    "for result in results:\n",
    "    log_it(result)\n",
    "\n",
    "duration = round(time.time() - start_time)\n",
    "log_it(f\"\\nDone {len(files)} items in {duration} seconds ({round(duration/60, 2)} min or {round(duration/3600, 2)} hours)\")"
   ]
  }
 ]
}